# Pgvectorscale RAG Solution with Python

This tutorial will guide you through setting up and using pgvectorscale with Docker, Python and Psycopg 3, using OpenAI's text-embedding-3-small model for embeddings. We will build a RAG (Retrieval-Augmented Generation) solution that includes both retrieval of relevant information and answer generation based on the retrieved context.

## Timescale Documentation

For more information about using PostgreSQL as a vector database in AI applications with Timescale, check out these resources:

- [Blog Post: A Python Library for Using PostgreSQL as a Vector Database in AI Applications](https://www.timescale.com/blog/a-python-library-for-using-postgresql-as-a-vector-database-in-ai-applications/)
- [GitHub Repository: pgvectorscale](https://github.com/timescale/pgvectorscale)


## Prerequisites

- Docker
- Python 3.7+
- OpenAI API key
- PostgreSQL GUI client

## Steps

1. Set up Docker environment
2. Connect to the database using a PostgreSQL GUI client (I use TablePlus)
3. Create a Python script to insert document chunks as vectors using OpenAI embeddings
4. Create a Python function to perform similarity search

## Detailed Instructions

### 1. Set up Docker environment

Create a `docker-compose.yml` file with the following content:

```yaml
services:
  timescaledb:
    image: timescale/timescaledb-ha:pg16
    container_name: timescaledb
    environment:
      - POSTGRES_DB=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - timescaledb_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    restart: unless-stopped

volumes:
  timescaledb_data:
```

Create an `init.sql` file in the same directory with the following content:

```sql
-- Enable the vectorscale extension
CREATE EXTENSION IF NOT EXISTS vectorscale CASCADE;

-- Create the embeddings table
CREATE TABLE IF NOT EXISTS embeddings (
    id BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
    question TEXT,
    answer TEXT,
    category TEXT,
    embedding VECTOR(1536)
);

-- Create the StreamingDiskANN index
CREATE INDEX IF NOT EXISTS embeddings_idx ON embeddings
USING diskann (embedding);
```

This SQL script does the following:

1. Activates the `vectorscale` extension, which provides vector similarity search capabilities.

2. Creates a table named `embeddings` with the following columns:
   - `id`: A unique identifier for each entry.
   - `question`: The FAQ question text.
   - `answer`: The corresponding answer text.
   - `category`: An additional field to categorize the FAQ entries.
   - `embedding`: A vector column to store the 1536-dimensional embedding of the question.

3. Creates a StreamingDiskANN index on the `embedding` column for efficient similarity searches.

This schema is designed to store FAQ documents, with separate columns for questions and answers, along with a category for additional organization. The embedding column will store the vector representation of the question, which will be used for similarity searches. You can update this depending on your data structure.

Run the Docker container:

```bash
docker compose up -d
```

### 2. Connect to the database using a PostgreSQL GUI client

- Open client
- Create a new connection with the following details:
  - Host: localhost
  - Port: 5432
  - User: postgres
  - Password: password
  - Database: postgres

### 3. Create a Python script to insert document chunks as vectors

See `insert_vectors.py` for the implementation. This script uses OpenAI's `text-embedding-3-small` model to generate embeddings.

### 4. Create a Python function to perform similarity search

See `similarity_search.py` for the implementation. This script also uses OpenAI's `text-embedding-3-small` model for query embedding.

## Usage

1. Create a copy of `example.env` and rename it to `.env`
2. Open `.env` and fill in your OpenAI API key. Leave the database settings as is
3. Run the Docker container
4. Install the required Python packages using `pip install -r requirements.txt`
5. Execute `insert_vectors.py` to populate the database
6. Play with `similarity_search.py` to perform similarity searches

## Cosine Similarity in Vector Search

### What is Cosine Similarity?

Cosine similarity measures the cosine of the angle between two vectors in a multi-dimensional space. It's a measure of orientation rather than magnitude.

- Range: -1 to 1 (for normalized vectors, which is typical in text embeddings)
- 1: Vectors point in the same direction (most similar)
- 0: Vectors are orthogonal (unrelated)
- -1: Vectors point in opposite directions (most dissimilar)

### Cosine Distance

In pgvector, the `<=>` operator computes cosine distance, which is 1 - cosine similarity.

- Range: 0 to 2
- 0: Identical vectors (most similar)
- 1: Orthogonal vectors
- 2: Opposite vectors (most dissimilar)

### Interpreting Results

When you get results from similarity_search:

- Lower distance values indicate higher similarity.
- A distance of 0 would mean exact match (rarely happens with embeddings).
- Distances closer to 0 indicate high similarity.
- Distances around 1 suggest little to no similarity.
- Distances approaching 2 indicate opposite meanings (rare in practice).
